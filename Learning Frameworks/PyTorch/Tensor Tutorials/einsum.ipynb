{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Basic operations w/ Einsum"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import torch\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# initalizing a tensor\r\n",
    "x = torch.rand((2,3))\r\n",
    "x"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0.9506, 0.0962, 0.4298],\n",
       "        [0.5465, 0.4349, 0.7661]])"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# premutation of Tensor\r\n",
    "torch.einsum(\"ij->ji\",x) # this here is a special case of Transposing (T)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0.9506, 0.5465],\n",
       "        [0.0962, 0.4349],\n",
       "        [0.4298, 0.7661]])"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# summation\r\n",
    "torch.einsum(\"ij->\",x)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(3.2241)"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# column sum\r\n",
    "torch.einsum(\"ij->j\",x)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([1.4971, 0.5310, 1.1960])"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# row sum\r\n",
    "torch.einsum(\"ij->i\",x)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([1.4766, 1.7475])"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# Matrix Vector Multiplication\r\n",
    "V = torch.rand((1,3))\r\n",
    "torch.einsum(\"ij, kj->ik\",x,V)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0.4631],\n",
       "        [0.9800]])"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "# Matrix - Matrix multiplication\r\n",
    "y = torch.rand((3,4))\r\n",
    "torch.einsum(\"ij, jk-> ik\",x,y)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0.8822, 1.0980, 1.0431, 0.8831],\n",
       "        [1.2143, 1.4004, 0.9319, 0.5482]])"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "# Dot product first row with first row of matrix\r\n",
    "torch.einsum(\"i, i->\",x[0],x[0])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(1.0977)"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "# Dot product with matrix\r\n",
    "torch.einsum(\"ij,ij->\",x,x)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(2.1724)"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "# Hadamard Product (element wise multiplication)\r\n",
    "torch.einsum(\"ij,ij->ij\",x,x)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0.9037, 0.0092, 0.1848],\n",
       "        [0.2987, 0.1891, 0.5869]])"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "# outer product\r\n",
    "a = torch.rand((3))\r\n",
    "b = torch.rand((5))\r\n",
    "torch.einsum(\"i,j->ij\",a,b)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[1.2311e-01, 3.6428e-01, 7.5051e-04, 5.3622e-01, 5.7032e-01],\n",
       "        [8.9691e-02, 2.6540e-01, 5.4680e-04, 3.9067e-01, 4.1552e-01],\n",
       "        [8.3478e-02, 2.4702e-01, 5.0892e-04, 3.6361e-01, 3.8674e-01]])"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "# Batch Matrix Multiplication\r\n",
    "a = torch.rand((3,2,5))\r\n",
    "b = torch.rand((3,5,3))\r\n",
    "torch.einsum(\"ijk,ikl->ijl\",a,b)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[1.4419, 1.3241, 0.9689],\n",
       "         [1.5320, 1.4319, 1.0265]],\n",
       "\n",
       "        [[0.5533, 0.9317, 0.2423],\n",
       "         [1.1477, 1.4001, 0.6014]],\n",
       "\n",
       "        [[1.1152, 0.7390, 1.4030],\n",
       "         [0.2587, 0.2661, 0.4532]]])"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "# Matrix diagonal\r\n",
    "x = torch.rand((3,3))\r\n",
    "torch.einsum(\"ii->i\",x)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([0.6895, 0.3006, 0.6306])"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "# Matrix trace\r\n",
    "torch.einsum(\"ii->\",x)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(1.6207)"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Case Studies"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TREEQN\r\n",
    "\r\n",
    "Given a low-dimensional state representation zl at layer l and a transition function W^a per action a, we want to calculate all next-state representations zal+1 using a residual connection.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "import torch.nn.functional as F\r\n",
    "\r\n",
    "def random_tensors(shape, num=1, requires_grad=False):\r\n",
    "  tensors = [torch.randn(shape, requires_grad=requires_grad) for i in range(0, num)]\r\n",
    "  return tensors[0] if num == 1 else tensors\r\n",
    "\r\n",
    "# Parameters\r\n",
    "# -- [num_actions x hidden_dimension]\r\n",
    "b = random_tensors([5, 3], requires_grad=True)\r\n",
    "# -- [num_actions x hidden_dimension x hidden_dimension]\r\n",
    "W = random_tensors([5, 3, 3], requires_grad=True)\r\n",
    "\r\n",
    "def transition(zl):\r\n",
    "  # -- [batch_size x num_actions x hidden_dimension]\r\n",
    "  return zl.unsqueeze(1) + F.tanh(torch.einsum(\"bk,aki->bai\", [zl, W]) + b)\r\n",
    "\r\n",
    "# Sampled dummy inputs\r\n",
    "# -- [batch_size x hidden_dimension]\r\n",
    "zl = random_tensors([2, 3])\r\n",
    "\r\n",
    "transition(zl)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[ 0.2614,  0.0652,  1.0455],\n",
       "         [-0.4112,  1.8931,  1.5323],\n",
       "         [ 0.4651,  1.9163,  1.5342],\n",
       "         [ 0.4624,  1.7946,  0.8379],\n",
       "         [-0.7446,  1.9776,  1.6927]],\n",
       "\n",
       "        [[ 0.4550, -0.2439,  0.7862],\n",
       "         [-0.6796,  1.2814, -0.5299],\n",
       "         [ 0.6686,  1.0737,  0.6388],\n",
       "         [ 0.4404,  1.1259,  0.5036],\n",
       "         [ 0.0452,  1.2452,  0.2531]]], grad_fn=<AddBackward0>)"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ATTENTION\r\n",
    "word-by-word attention mechanism."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "# Parameters\r\n",
    "# -- [hidden_dimension]\r\n",
    "bM, br, w = random_tensors([7], num=3, requires_grad=True)\r\n",
    "# -- [hidden_dimension x hidden_dimension]\r\n",
    "WY, Wh, Wr, Wt = random_tensors([7, 7], num=4, requires_grad=True)\r\n",
    "\r\n",
    "# Single application of attention mechanism \r\n",
    "def attention(Y, ht, rt1):\r\n",
    "  # -- [batch_size x hidden_dimension] \r\n",
    "  tmp = torch.einsum(\"ik,kl->il\", [ht, Wh]) + torch.einsum(\"ik,kl->il\", [rt1, Wr])\r\n",
    "  Mt = F.tanh(torch.einsum(\"ijk,kl->ijl\", [Y, WY]) + tmp.unsqueeze(1).expand_as(Y) + bM)\r\n",
    "  # -- [batch_size x sequence_length]\r\n",
    "  at = F.softmax(torch.einsum(\"ijk,k->ij\", [Mt, w])) \r\n",
    "  # -- [batch_size x hidden_dimension]\r\n",
    "  rt = torch.einsum(\"ijk,ij->ik\", [Y, at]) + F.tanh(torch.einsum(\"ij,jk->ik\", [rt1, Wt]) + br)\r\n",
    "  # -- [batch_size x hidden_dimension], [batch_size x sequence_dimension]\r\n",
    "  return rt, at\r\n",
    "\r\n",
    "# Sampled dummy inputs\r\n",
    "# -- [batch_size x sequence_length x hidden_dimension]\r\n",
    "Y = random_tensors([3, 5, 7])\r\n",
    "# -- [batch_size x hidden_dimension]\r\n",
    "ht, rt1 = random_tensors([3, 7], num=2)\r\n",
    "\r\n",
    "rt, at = attention(Y, ht, rt1)\r\n",
    "at  # -- print attention weights"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\MANNPA~1\\AppData\\Local\\Temp/ipykernel_21416/3456548744.py:13: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  at = F.softmax(torch.einsum(\"ijk,k->ij\", [Mt, w]))\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0.0656, 0.2419, 0.2439, 0.3282, 0.1205],\n",
       "        [0.3106, 0.5143, 0.0332, 0.0881, 0.0538],\n",
       "        [0.1002, 0.1683, 0.0513, 0.3149, 0.3654]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('torch': conda)"
  },
  "interpreter": {
   "hash": "cce4b1c5724572779e903609fcfbc61702edc1fd05188be047d442d011fa2c05"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}