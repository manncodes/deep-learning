{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we assume that the relationship\n",
    "between features $\\mathbf{x}$ and target $y$\n",
    "is approximately linear,\n",
    "i.e., that the conditional mean $E[Y \\mid X=\\mathbf{x}]$\n",
    "can be expressed as a weighted sum\n",
    "of the features $\\mathbf{x}$.\n",
    "This setup allows that the target value\n",
    "may still deviate from its expected value\n",
    "on account of observation noise.\n",
    "Next, we can impose the assumption that any such noise\n",
    "is well-behaved, following a Gaussian distribution.\n",
    "Typically, we will use $n$ to denote\n",
    "the number of examples in our dataset.\n",
    "We use superscripts to enumerate samples and targets,\n",
    "and subscripts to index coordinates.\n",
    "More concretely,\n",
    "$\\mathbf{x}^{(i)}$ denotes the $i$-th sample\n",
    "and $x_j^{(i)}$ denotes its $j$-th coordinate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate synthetic data\n",
    "class RegressionData():\n",
    "    def __init__(self, w, b, noise=0.01, num_train=1000, num_val=1000, batch_size=32):\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "        self.num_train = num_train\n",
    "        self.num_val = num_val\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.n = self.num_train + self.num_val\n",
    "        self.X = torch.randn(self.n, self.w.shape[0])\n",
    "        self.noise = torch.randn(self.n, 1) * noise\n",
    "        self.Y = self.X @ w.reshape(-1, 1) + b + self.noise\n",
    "\n",
    "        self.X_train, self.Y_train = self.X[:self.num_train], self.Y[:self.num_train]\n",
    "        self.X_val, self.Y_val = self.X[self.num_train:], self.Y[self.num_train:]\n",
    "\n",
    "    def get_data(self, batch_size, train=True):\n",
    "        return (self.X_train, self.Y_train) if train else (self.X_val, self.Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = RegressionData(\n",
    "    w = torch.tensor([2, -3.4]),\n",
    "    b = 4.2,\n",
    ")\n",
    "\n",
    "X_train, Y_train = gen.get_data(gen.batch_size, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torch.utils.data.TensorDataset(X_train, Y_train)\n",
    "data_iter = torch.utils.data.DataLoader(dataset, batch_size=gen.batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset), len(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, num_inputs, sigma=0.01):\n",
    "        super().__init__()\n",
    "        self.w = torch.normal(0, sigma, size=(num_inputs, 1), requires_grad=True)\n",
    "        self.b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return X @ self.w + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearModel(num_inputs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(torch.optim.Optimizer):\n",
    "    def __init__(self, params, lr):\n",
    "        defaults = dict(lr=lr)\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"\n",
    "        Performs a single optimization step.\n",
    "        \"\"\"\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                p.data -= group['lr'] * p.grad.data\n",
    "\n",
    "    def zero_grad(self):\n",
    "        \"\"\"\n",
    "        clear the gradients of all optimized parameters\n",
    "        \"\"\"\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                p.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss\n",
    "def squared_loss(y_hat, y):\n",
    "    return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimization\n",
    "def train(model, data_iter, loss, optimizer, num_epochs):\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        for X, y in data_iter:\n",
    "            y_hat = model(X)\n",
    "            l = loss(y_hat, y)\n",
    "            l.sum().backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        print(f'epoch {epoch}, loss: {l.mean():f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss: 0.000066\n",
      "epoch 2, loss: 0.000044\n",
      "epoch 3, loss: 0.000020\n"
     ]
    }
   ],
   "source": [
    "train(model, data_iter, squared_loss, SGD([model.w, model.b], lr=0.03), num_epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error in estimating w: tensor([-0.0002, -0.0008])\n",
      "error in estimating b: tensor([0.0021])\n"
     ]
    }
   ],
   "source": [
    "W_true = gen.w\n",
    "b_true = gen.b\n",
    "\n",
    "W_model = model.w\n",
    "b_model = model.b\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(f\"error in estimating w: {W_true - W_model.reshape(-1)}\")\n",
    "    print(f\"error in estimating b: {b_true - b_model.reshape(-1)}\")\n",
    "\n",
    "# Here we have used `torch.no_grad()` to avoid tracking the gradient. \n",
    "# When performing any external operations outside of training loop, \n",
    "# we don't want to accumulate the gradient of those operations. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d21cb133c8b9d183623f3b8d56ae80a99ba33a1f55739365632c04feb6139c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
