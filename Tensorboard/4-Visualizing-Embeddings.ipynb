{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit"
  },
  "interpreter": {
   "hash": "e2e7150d6fef36d5f82e9d43d35d088b66b4d59dae97ad4a61af3cc04b096018"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter # to print to tensorboard\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple CNN\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, in_channels=1, num_classes=10):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=in_channels, out_channels=8, kernel_size=3, stride=1, padding=1\n",
    "        )\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=8, out_channels=16, kernel_size=3, stride=1, padding=1\n",
    "        )\n",
    "        self.fc1 = nn.Linear(16 * 7 * 7, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "# learning_rate = 0.001\n",
    "in_channels = 1\n",
    "num_classes = 10\n",
    "# batch_size = 64\n",
    "num_epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "train_dataset = datasets.MNIST(\n",
    "    root='dataset/',\n",
    "    train=True,\n",
    "    transform=transforms.ToTensor(),\n",
    "    download=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes = [256]\n",
    "learning_rates = [0.01]\n",
    "classes = [str(x) for x in range(10)] #class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "235it [00:44,  5.24it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for batch_size in batch_sizes:\n",
    "    for learning_rate in learning_rates:\n",
    "        step = 0\n",
    "\n",
    "        ## move elements inside the loop which depends on changing hyperparameters\n",
    "\n",
    "        #new trainloader according to batch size\n",
    "        train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size,shuffle=True)\n",
    "        \n",
    "        #initialize model\n",
    "        model = CNN(in_channels=in_channels,num_classes=num_classes).to(device=device)\n",
    "        model.train()\n",
    "        \n",
    "        # initialize summaryWriter for each combo\n",
    "        writer = SummaryWriter(f\"runs/MNIST/MiniBatchSize-{batch_size}-LR-{learning_rate}\")\n",
    "\n",
    "        # new loss_fn and optimizer with changed hyperparamters\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(),lr=learning_rate, weight_decay=0.0)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            losses = []\n",
    "            accuracies = []\n",
    "            for batch_idx, (data, targets) in tqdm(enumerate(train_loader)):\n",
    "                data = data.to(device)\n",
    "                targets = targets.to(device)\n",
    "\n",
    "                #forward\n",
    "                score = model(data)\n",
    "                loss = loss_fn(score,targets)\n",
    "                losses.append(loss.item())\n",
    "\n",
    "                #backward\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "\n",
    "                #optimizer step\n",
    "                optimizer.step()\n",
    "\n",
    "\n",
    "                #Calculate 'running accuracy'\n",
    "                _,predictions = score.max(1)\n",
    "                num_correct = (predictions==targets).sum()\n",
    "                running_train_acc = float(num_correct)/float(data.shape[0])\n",
    "                accuracies.append(running_train_acc)\n",
    "                \n",
    "                ### Plotting to tensorboard \n",
    "                # Plotting batch image grid\n",
    "                img_grid = torchvision.utils.make_grid(data)\n",
    "                writer.add_image('mnist_images', img_grid,global_step=step)\n",
    "\n",
    "                #writing weights of any layer like fc1 or conv1\n",
    "                writer.add_histogram('fc1',model.fc1.weight,global_step=step)\n",
    "                writer.add_histogram('conv1',model.conv1.weight,global_step=step)\n",
    "\n",
    "                 \n",
    "                # plotting loss & acc to tensorboard\n",
    "                writer.add_scalar('Training loss',loss,global_step=step)\n",
    "                writer.add_scalar('Training accuracy:',running_train_acc,global_step=step)\n",
    "\n",
    "                # plotting embdeddings \n",
    "\n",
    "                if batch_idx == 233:\n",
    "                    features = data.reshape(data.shape[0],-1)\n",
    "                    class_labels = [classes[label] for label in predictions]\n",
    "                    writer.add_embedding(\n",
    "                        features,\n",
    "                        metadata=class_labels,\n",
    "                        label_img = data,\n",
    "                        global_step=batch_idx)\n",
    "\n",
    "                \n",
    "                #update global step\n",
    "                step+=1\n",
    "            \n",
    "            # write hparams after each batch\n",
    "            writer.add_hparams({'lr': learning_rate,'bsize':batch_size},\n",
    "                               {'accuracy': sum(accuracies)/len(accuracies),\n",
    "                                'loss':sum(losses)/len(losses)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check tensorboard by 'python -m tensorboard.main --logdir runs'"
   ]
  }
 ]
}