{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proximal Policy Optimization - PPO\n",
    "PPO is a policy gradient method for reinforcement learning. Simple policy gradient methods do a single gradient update per sample (or a set of samples). Doing multiple gradient steps for a single sample causes problems because the policy deviates too much, producing a bad policy. PPO lets us do multiple gradient updates per sample by trying to keep the policy close to the policy that was used to sample data. It does so by clipping gradient flow if the updated policy is not close to the policy used to sample the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Policy gradient loss\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(PPOLoss, self).__init__()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        log_pi: torch.Tensor,\n",
    "        sampled_log_pi: torch.Tensor,\n",
    "        advantage: torch.Tensor,\n",
    "        clip: float,\n",
    "    ):\n",
    "        ratio = torch.exp(log_pi - sampled_log_pi)\n",
    "        clipped_ratio = torch.clamp(ratio, 1 - clip, 1 + clip)\n",
    "        policy_reward = torch.min(ratio * advantage, clipped_ratio * advantage)\n",
    "        self.clip_fraction = (abs(ratio - 1.0) > clip).to(torch.float).mean()\n",
    "        return -policy_reward.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClippedValueFunctionLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ClippedValueFunctionLoss, self).__init__()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        value: torch.Tensor,\n",
    "        sampled_value: torch.Tensor,\n",
    "        sampled_return: torch.Tensor,\n",
    "        clip: float,\n",
    "    ):\n",
    "        clipped_value = sampled_value + (value - sampled_value).clamp(-clip, clip)\n",
    "        vf_loss = torch.max(\n",
    "            (value - sampled_return) ** 2, (clipped_value - sampled_return) ** 2\n",
    "        )\n",
    "        return 0.5 * vf_loss.mean()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cef9e06bb236b2a8629b07e87a04b187b952a0f661eff5533360a155783f0c33"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
